# -*- coding: utf-8 -*-
"""LDGRP

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/mannangolchha/ldgrp.7b91dddd-c1d8-4721-8206-95fcc0bd1334.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250803/auto/storage/goog4_request%26X-Goog-Date%3D20250803T122607Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D199724d486714df0003b0bc3d2d51f825b7752b52ba087f2587e0e47e8f9f1126a31e521db2111a5f35d6def30c39786984e743c802c0c1c8e22dacb4c8b5431c2a276fb9fc7854f7b56d6c52059ff65d16b770bebfcdf4a4c97276f673d3004d7fa6524ff7ceedf63d9baddcdaae12a6e96d2567e0a1fc28572a78c9ca8ebb504fbc4a4f47b32a7ce194e7e7cc5bdac70ce55de24f274cdcf5bbd842016c949858c9202d1f63aeb1695362391a9db5f0f94653bb38c44636b9667f1669637b7cf4e11901613a4b297c3285f6548fce3d47ff3004f37269a386b2fbe034ce284f073781207e9af678a7f23b7cfdf7f94124bf517c22e1b6c75ccfa64d3e3ca37
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

landmark_recognition_2021_path = kagglehub.competition_download('landmark-recognition-2021')

print('Data source import complete.')

# https://www.kaggle.com/c/landmark-recognition-2021/data?select=train.csv

import pandas as pd
import numpy as np
import os
import tensorflow as tf
import tensorflow.keras as keras
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras import regularizers
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
from google.colab.patches import cv2_imshow
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix
import sys
import gc
import time
seed = 3

from sklearn.utils import class_weight

def load_data() :
  train_df = pd.read_csv('/kaggle/input/landmark-recognition-2021/train.csv')
  train_df['img_path'] = train_df['id'].map(lambda r : os.path.join('/kaggle/input/landmark-recognition-2021/train', r[0], r[1], r[2], r + '.jpg'))
  train_df['landmark_id'] = train_df['landmark_id'].apply(lambda x : np.int32(x))
  return train_df

def read_resize(img) :
  arr = cv2.imread(img)
  image = cv2.resize(arr, (IMG_SIZE, IMG_SIZE))
  return image

def get_size(obj, seen = None) :
  """ Recursively finds size of objects """
  size = sys.getsizeof(obj)
  if seen is None:
    seen = set()

  obj_id = id(obj)
  if obj_id in seen :
    return 0
  seen.add(obj_id)
  if isinstance(obj, dict) :
    size += sum([get_size(v, seen) for v in obj.values()])
    size += sum([get_size(k, seen) for k in obj.keys()])
  elif hasattr(obj, '__dict__') :
    size += get_size(obj.__dict__, seen)
  elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)) :
    size += sum([get_size(i, seen) for i in obj])
  return size

traindf = load_data()
print(traindf['landmark_id'].nunique())
traindf.head()

get_size(traindf)

# no. of images
traindf.shape[0]

traindf['landmark_id'].value_counts()

plt.figure(figsize= (25,10))
for i in range(10) :
  num = np.random.randint(traindf.shape[0])
  plt.subplot(4,5,i+1)
  im = plt.imread(traindf['img_path'][num])
  plt.xticks([])
  plt.yticks([])
  plt.xlabel(str(im.shape[0]) + 'x' + str(im.shape[1]))
  plt.imshow(im)
plt.show()

IMG_SIZE = 128
min_class = 0
max_class = 500
traindf = traindf[(traindf['landmark_id'] > min_class) & (traindf['landmark_id'] < max_class)]
undersampling_threshold = 80
traindf = traindf.groupby('landmark_id', group_keys=False).apply(lambda x : x.sample(n = min(undersampling_threshold, len(x)), random_state = seed))
traindf.reset_index(drop=True, inplace=True)
n_classes = traindf['landmark_id'].unique()
print(f"No. of classes : {traindf['landmark_id'].nunique()}")
print(traindf['landmark_id'].value_counts())

plt.figure(figsize= (25,10))
for i in range(10) :
  num = np.random.randint(traindf.shape[0])
  plt.subplot(4,5,i+1)
  im = read_resize(traindf['img_path'][num])
  plt.xticks([])
  plt.yticks([])
  plt.xlabel(traindf['landmark_id'][num])
  plt.imshow(im)
plt.show()

X = [] # images
y = [] # labels
for i in range(traindf.shape[0]) :
  img = read_resize(traindf['img_path'][i])
  X.append(img)
  y.append(traindf['landmark_id'][i])

classes = traindf['landmark_id'].unique()
le = LabelEncoder()
le.fit(classes)
y_le = le.transform(y)

n_c = len(le.classes_)
n_c

X = [n/255 for n in X]
X= np.array(X)
y = np.array(y)
y_le = np.array(y_le)

xtrain, xval, ytrain, yval = train_test_split(X, y_le, test_size = 0.1, random_state = seed, shuffle = True)
xtrain, xtest, ytrain, ytest = train_test_split(xtrain, ytrain, test_size = 0.01, random_state = seed, shuffle = True)
len(xtrain), len(xtest), len(xval), len(ytrain)

data_augmentation = tf.keras.Sequential([
   # layers.RandomFlip('horizontal'),
   # layers.RandomCrop(120, 120, seed = seed),
    layers.RandomRotation(factor = (-0.1, 0.1)),
    layers.RandomZoom(height_factor = (-0.2, 0.2)),
    layers.RandomTranslation(height_factor = (-0.2, 0.2), width_factor = (-0.2, 0.2)),
    layers.RandomContrast(0.4)
])

plt.figure(figsize = (25, 7))
img = X[1]
plt.imshow(img)
plt.axis('off')
plt.xlabel("Original Image")

for i in range(5) :
  mod = data_augmentation(img)
  plt.subplot(1, 5, i+1)
  plt.imshow(mod)
  plt.axis('off')
  plt.xlabel("Augmented Image")
plt.show()

oversampling_threshold = 15
for i in range(n_c):
    # Count samples for class i
    class_mask = (ytrain == i)  # Remove int() conversion
    count = np.sum(class_mask)

    if count < oversampling_threshold:
        # Get all images of class i
        class_images = xtrain[class_mask]

        # Skip if no samples found (safety check)
        if len(class_images) == 0:
            print(f"Warning: Class {i} has 0 samples. Skipping.")
            continue

        # Randomly sample to reach threshold
        num_to_add = oversampling_threshold - count
        random_indices = np.random.choice(len(class_images), size=num_to_add)  # Safer than randint
        new_images = class_images[random_indices]

        # Append new samples
        xtrain = np.concatenate((xtrain, new_images), axis=0)
        ytrain = np.append(ytrain, [i] * num_to_add)

print(f"Final dataset size: {len(xtrain)}")

# CNN Hyperparameters
kernelsize = (3,3)
stridesize = (2,2)
activation = 'relu'
poolsize = (2,2)
padding = 'same'
regularizer = regularizers.l2(0.01)
epochs = 1000
batchsize = 256
lr = 0.01

def create_model() :
  model = keras.Sequential([
      # Data Augmentation
      layers.RandomRotation(factor = (-0.1, 0.1)),
      layers.RandomZoom(height_factor = (-0.2, 0.2)),
      layers.RandomTranslation(height_factor = (-0.2, 0.2), width_factor = (-0.2, 0.2)),
      layers.RandomContrast(0.4),

      # Feature Extraction
      layers.Conv2D(32, kernel_size = kernelsize, activation = activation, padding = padding, strides = 1, input_shape = [IMG_SIZE, IMG_SIZE, 3]),
      layers.BatchNormalization(),
      layers.MaxPooling2D(pool_size = poolsize, strides = stridesize, padding = padding),

      layers.Conv2D(64, kernel_size = kernelsize, activation = activation, padding = padding, strides = 1),
      layers.BatchNormalization(),
      layers.MaxPooling2D(pool_size = poolsize, strides = stridesize, padding = padding),

      layers.Conv2D(128, kernel_size = kernelsize, activation = activation, padding = padding, strides = 1),
      layers.BatchNormalization(),
      layers.MaxPooling2D(pool_size = poolsize, strides = stridesize, padding = padding),

      layers.Conv2D(256, kernel_size = kernelsize, activation = activation, padding = padding, strides = 1),
      layers.BatchNormalization(),
      layers.MaxPooling2D(pool_size = poolsize, strides = stridesize, padding = padding),

      layers.Conv2D(512, kernel_size = kernelsize, activation = activation, padding = padding, strides = 1),
      layers.BatchNormalization(),
      layers.MaxPooling2D(pool_size = poolsize, strides = stridesize, padding = padding),

      # model head
      layers.Flatten(),
      layers.Dense(512, activation = activation, kernel_regularizer = regularizer),
      layers.BatchNormalization(),

      layers.Dense(n_c, activation = 'softmax')
  ])

  # Optimizers
  opt1 = keras.optimizers.Adam(learning_rate = lr, beta_1 = 0.9, beta_2 = 0.999, epsilon=1e-7)
  opt2 = keras.optimizers.RMSprop(learning_rate = lr, rho = 0.9, momentum = 0.5, epsilon=1e-7)
  opt3 = keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True)

  model.compile(optimizer = opt3, loss = 'sparse_categorical_crossentropy', metrics = ['sparse_categorical_accuracy'])

  # callbacks
  es = EarlyStopping(monitor = 'val_loss', patience=150, min_delta = 0.0001, mode = 'auto', restore_best_weights=True)
  reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, patience = 40, min_delta = 0.001, cooldown = 1, min_lr = 0.005, verbose = 1)
  mc = ModelCheckpoint('best.weights.h5', save_best_only=True, save_weights_only=True, monitor = 'val_loss')

  # training model
  history = model.fit(
      xtrain, ytrain,
      validation_data = (xval, yval),
      epochs = epochs,
      batch_size = batchsize,
      steps_per_epoch = len(xtrain) // batchsize,
      callbacks = [es, reduce_lr, mc],
      verbose = 1,
      shuffle = True,
      class_weight = train_classWeights,
      # use_multiprocessing = True,
      # workers = 32,
      # max_queue_size = 15  # default : 10
  )
  return model, history

classwts = class_weight.compute_class_weight(class_weight = 'balanced', classes = np.unique(ytrain), y = ytrain)
train_classWeights = dict(enumerate(classwts))

model, history = create_model()
plt.figure(figsize = (15, 5))
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label = 'Training Loss')
plt.plot(history.history['val_loss'], label = 'Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['sparse_categorical_accuracy'], label = 'Training Accuracy')
plt.plot(history.history['val_sparse_categorical_accuracy'], label = 'Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.show()

model.summary()

score = model.evaluate(xtest, ytest)
print(f"Test Loss : {score[0]}")
print(f"Test Accuracy : {score[1]}")

plt.figure(figsize = (25, 7))
for i in range(10) :
  plt.subplot(2,5,i+1)
  plt.imshow(xtest[i])
  plt.xlabel(ytest[i])
  plt.yticks([])
  plt.xticks([])
plt.show()

ypred = model.predict(xtest, verbose = 1)

y_pred = []
confidence = []
for i in range(len(ypred)) :
  y_pred.append(np.argmax(ypred[i]))
  confidence.append((np.max(ypred[i])).round(2))
y_pred = le.inverse_transform(y_pred)
ytest = le.inverse_transform(ytest)

# display results
random_test_img_idx = np.random.choice(len(xtest), 10, replace=False)  # Get 10 unique random indices
fig, ax = plt.subplots(10, 5, figsize=(25, 40))  # 10 rows (test images), 5 columns (test + 4 examples)

for row, img_idx in enumerate(random_test_img_idx):
    predicted = y_pred[img_idx]
    true = ytest[img_idx]
    confidence = ypred[img_idx].max() if hasattr(ypred[img_idx], 'max') else ypred[img_idx]  # Handle confidence

    # Display test image to predict (first column)
    ax[row, 0].imshow(xtest[img_idx])
    ax[row, 0].set_xticks([])
    ax[row, 0].set_yticks([])
    ax[row, 0].set_title(f"Image to predict #{img_idx}")
    ax[row, 0].set_xlabel(f"Predicted: {predicted}\nTrue: {true}\nConfidence: {confidence:.2f}")

    # Display 4 training examples of the true class (columns 2-5)
    class_examples = xtrain[ytrain == true][:4]
    for col in range(1, 5):
        if col-1 < len(class_examples):  # Check if example exists
            ax[row, col].imshow(class_examples[col-1])
            ax[row, col].set_xticks([])
            ax[row, col].set_yticks([])
            ax[row, col].set_xlabel(f"Image of class {true}")
        else:
            ax[row, col].axis('off')  # Hide if no example available

plt.tight_layout()
plt.show()

"""<font size = 4>__label 12 images__"""

p = traindf[traindf['landmark_id'] == 12]['img_path']
p = list(p)

print(f"Total images: {len(p)}")

rows = 4
cols = 6
max_images = rows * cols  # 24

plt.figure(figsize=(25, 10))

for i in range(min(len(p), max_images)):
    plt.subplot(rows, cols, i+1)  # Positions 1-24
    img = read_resize(p[i])
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img)

plt.tight_layout()
plt.show()